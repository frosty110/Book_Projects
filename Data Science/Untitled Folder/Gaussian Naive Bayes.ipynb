{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes are a set of <b>supervised</b> learning algorithms based on Bayes' theorem with the naive assumption of independence between every pair of feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Bayes Theorem:</b>\n",
    "<br>    Given a class variable $y$ and a dependent feature vector $x_1,..., x_n$, Bayes theorem states the following relationship:\n",
    "$$P(y|x_1,...,x_n) = \\frac{P(y)P(x_1,...x_n|y)}{P(x_1,...,x_n)}$$\n",
    "This is dervied from the prior and posterior probability.\n",
    "<b><i><p>Cancer Example</b></i>. \n",
    "<br>$P(C)=.001$ \n",
    "<br>Test has a 90% it's positive if C they have cancer. <b> sensitivity </b>. \n",
    "<br>There's a 90% it's negative if C they don't have cancer. <b> Specitivity</b>. \n",
    "<br>What is the the probability of having cancer? \n",
    "<br>Draw a diagram. We have 100,000 people. 1% has cancer so 1,000 people have cancer. We have a test that 90% positive they have cancer. So of that 1,000 about 90% of 900 will be tested positive. However, the test will show 90% don't have the  cancer which is 90,000 meaning that 10% or 10,000 got a positive of the test. Of the test that showed positive, only a portion of that 1,000 that have the cancer is in that 10,000 which mathematically is roughly 8.3:\n",
    "<br> Prior probability * Test evidence -> posterior pobability\n",
    "\n",
    "<b>prior</b>: \n",
    "<br> $P(C) = 0.01$\n",
    "<br> $P(Pos|C)=.9$\n",
    "\n",
    "<b>Joint</b>: \n",
    "<br> $P(C|Pos) = P(C)\\cdot{P(Pos|C)}$ = .009. This is the total cancer probability in the positive test\n",
    "<br> $P(\\neg C|Pos)=P(\\neg C)\\cdot{P}(Pos|\\neg C)$ = $.99\\cdot{.1}$ = .099. This is the total positive test.\n",
    "\n",
    "<b>Normalization</b>: \n",
    "<br> $P(Pos)=P(C|Pos)+P(\\neg C|Pos)=.108$\n",
    "\n",
    "<b>Posterior</b>: \n",
    "<br> $P(C |Pos)= \\frac{.009}{.108} = .0833$\n",
    "<br> $P(\\neg C |Pos)=\\frac{.099}{.108} = .9167$\n",
    "<br>  $P(C |Pos) + P(\\neg C |Pos) = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Gaussian Naive bayes: </b>\n",
    "<br>The likelihood of the features is assimed to be Gaussian:\n",
    "$$P(x_i, y)=\\frac{1}{\\sqrt{2\\pi\\sigma^2_y}}exp \\left( -\\frac{(x_i-\\mu_y)^2}{2\\sigma^2_y}\\right)$$\n",
    "where $\\sigma_y$ and $\\mu_y$ are estimated using maximum likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "1.0\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# training points\n",
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "Y = np.array([1, 1, 1, 2, 2, 2])\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "# create our classifier\n",
    "clf = GaussianNB()\n",
    "# feed the training data where x are the features, and y are the labels.\n",
    "clf.fit(X, Y)\n",
    "\n",
    "# find which class this point belongs to.\n",
    "print(clf.predict([[-0.8, -1]]))\n",
    "print(clf.score(X,Y))\n",
    "\n",
    "\n",
    "# partial fit\n",
    "clf_pf = GaussianNB()\n",
    "clf_pf.partial_fit(X, Y, np.unique(Y))\n",
    "\n",
    "print(clf_pf.predict([[-0.8, -1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Multinomial naive Bayes:</b>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> bernoulli Naive Bayes: </b>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
